import streamlit as st
from src.agent import app as devmanual_ai_app


# --- í˜ì´ì§€ ì„¤ì • ---
st.set_page_config(
    page_title="DevManual-AI",
    page_icon="ğŸ¤–"
)
st.title("ğŸ‘¨â€ğŸ’» DevManual-AI")
st.caption("RAGì™€ LangGraph ê¸°ë°˜ì˜ ê¸°ìˆ  ë¬¸ì„œ ë¶„ì„ ë° ì½”ë“œ ìƒì„± AI ì—ì´ì „íŠ¸")

# ì‚¬ìš©ì ì§ˆë¬¸ ì…ë ¥
if "messages" not in st.session_state:
    st.session_state.messages = [
        {"role": "assistant", "content": "ì•ˆë…•í•˜ì„¸ìš”! DevManual-AIì…ë‹ˆë‹¤. ê¸°ìˆ  ë¬¸ì„œì— ëŒ€í•´ ê¶ê¸ˆí•œ ì ì´ë‚˜ ì½”ë“œ ìƒì„±ì´ í•„ìš”í•œ ë¶€ë¶„ì´ ìˆë‹¤ë©´ ë¬´ì—‡ì´ë“  ë¬¼ì–´ë³´ì„¸ìš”."}
    ]

# ì´ì „ ëŒ€í™” ë‚´ìš© í‘œì‹œ
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])

if prompt := st.chat_input("ê¶ê¸ˆí•œ ê¸°ìˆ ì´ë‚˜ ì½”ë“œì— ëŒ€í•´ ì§ˆë¬¸í•´ë³´ì„¸ìš”!"):
    # 1. ì‚¬ìš©ì ë©”ì‹œì§€ë¥¼ ëŒ€í™” ê¸°ë¡ì— ì¶”ê°€í•˜ê³  í™”ë©´ì— í‘œì‹œ
    st.session_state.messages.append({"role": "user", "content": prompt})
    with st.chat_message("user"):
        st.markdown(prompt)

    # 2. AI ì‘ë‹µ ìƒì„± ë° í‘œì‹œ
    with st.chat_message("assistant"):
        with st.spinner("AIê°€ ìƒê° ì¤‘ì…ë‹ˆë‹¤..."):
            # LangGraph í˜¸ì¶œ
            inputs = {"question": prompt}
            response_generator = devmanual_ai_app.stream(inputs)

            # ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µì„ ì‹¤ì‹œê°„ìœ¼ë¡œ í‘œì‹œ
            full_response = ""
            message_placeholder = st.empty()
            for chunk in response_generator:
                if "answer" in chunk.get("rag_node", {}):
                    full_response += chunk["rag_node"]["answer"]
                    message_placeholder.markdown(full_response + "â–Œ")
                elif "answer" in chunk.get("code_generation_node", {}):
                    full_response += chunk["code_generation_node"]["answer"]
                    message_placeholder.markdown(full_response + "â–Œ")
            
            message_placeholder.markdown(full_response)

    st.session_state.messages.append({"role": "assistant", "content": full_response})