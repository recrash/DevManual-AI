import os
import streamlit as st
from dotenv import load_dotenv

from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import RunnablePassthrough
from langchain_openai import AzureChatOpenAI, AzureOpenAIEmbeddings
from langchain_community.vectorstores import FAISS

# .env ë¡œë“œ
load_dotenv()


# ë²¡í„°DB ë¡œë“œ
def load_retriever():
    embeddings = AzureOpenAIEmbeddings(
        azure_endpoint=os.getenv("AOAI_ENDPOINT"),
        api_key=os.getenv("AOAI_API_KEY"),
        azure_deployment=os.getenv("AOAI_DEPLOY_EMBED_3_SMALL"),
        api_version="2024-02-01",
    )

    #faiss_index ì§€ì •
    db = FAISS.load_local("scripts/faiss_index", embeddings, allow_dangerous_deserialization=True)

    return db.as_retriever()


# LLMì—ê²Œ RAG ìˆ˜í–‰ì„ ì§€ì‹œí•˜ëŠ” í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ì •ì˜
def get_rag_prompt():
    template = """
    ë„ˆëŠ” ì‚¬ìš©ìì˜ ê¸°ìˆ  ì§ˆë¬¸ì— ëŒ€í•´ ì¹œì ˆí•˜ê³  ëª…í™•í•˜ê²Œ ë‹µë³€í•´ì£¼ëŠ” AI ì–´ì‹œìŠ¤í„´íŠ¸ 'DevManual-AI'ì•¼.
    ì œì‹œëœ [Context] ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ, ì‚¬ìš©ìì˜ [Question]ì— ëŒ€í•´ ë‹µë³€í•´ì¤˜.

    [Context]:
    {context}

    [Question]:
    {question}
    """
    return ChatPromptTemplate.from_template(template)

# Streamlit Settings
st.set_page_config(page_title="DevManual-AI", page_icon="ğŸ¤–")
st.title("ğŸ¤– DevManual-AI")
st.caption("ìŠ¤ë§ˆíŠ¸ ê¸°ìˆ  ë¬¸ì„œ ë¶„ì„ ë° ì½”ë“œ ìƒì„± ë´‡")

# ì‚¬ìš©ì ì§ˆë¬¸ ì…ë ¥
if "messages" not in st.session_state:
    st.session_state.messages = []


# ì´ì „ ëŒ€í™” ë‚´ìš© í‘œì‹œ
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])

if prompt := st.chat_input("ê¶ê¸ˆí•œ ê¸°ìˆ ì´ë‚˜ ì½”ë“œì— ëŒ€í•´ ì§ˆë¬¸í•´ë³´ì„¸ìš”!"):
    st.session_state.messages.append({"role": "user", "content": prompt})
    with st.chat_message("user"):
        st.markdown(prompt)

    with st.chat_message("assistant"):
        # RAG ì²´ì¸ ì‹¤í–‰ ë° ë‹µë³€ ìƒì„±
        with st.spinner("ë‹µë³€ì„ ìƒì„±í•˜ëŠ” ì¤‘ì…ë‹ˆë‹¤..."):
            retriever = load_retriever()
            prompt_template = get_rag_prompt()

            # Azure OpenAI LLM ëª¨ë¸ ì„¤ì •
            llm = AzureChatOpenAI(
                azure_endpoint=os.getenv("AOAI_ENDPOINT"),
                api_key=os.getenv("AOAI_API_KEY"),
                azure_deployment=os.getenv("AOAI_DEPLOY_GPT4O_MINI"), 
                api_version="2024-02-01",
                temperature=0.7 # ì•½ê°„ì˜ ì°½ì˜ì„±ì„ ë¶€ì—¬
            )

            # RAG ì²´ì¸ êµ¬ì„±(ì„¤ê³„ë„)
            rag_chain = (
                {"context": retriever, "question": RunnablePassthrough()}
                | prompt_template
                | llm
                | StrOutputParser()
            )

            # invokeê°€ ì‹¤í–‰ë˜ëŠ” ìˆœê°„ ìœ„ì—ì„œ ì„ ì–¸í•´ë†¨ë˜ íŒŒì´í”„ë¼ì¸ì´ ì‹¤í–‰ë˜ë©° ì¸ìŠ¤í„´ìŠ¤ê°€ ëœë‹¤.
            response = rag_chain.invoke(prompt)
            st.markdown(response)
    st.session_state.messages.append({"role": "assistant", "content": response})